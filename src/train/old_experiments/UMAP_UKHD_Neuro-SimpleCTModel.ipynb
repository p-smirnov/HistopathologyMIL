{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p163v/mambaforge/envs/marugoto/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/p163v/mambaforge/envs/marugoto/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/p163v/mambaforge/envs/marugoto/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/p163v/mambaforge/envs/marugoto/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import re\n",
    "import sklearn\n",
    "import umap\n",
    "\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "from torch import optim, utils, Tensor\n",
    "\n",
    "\n",
    "\n",
    "# from AttentionMIL_model import Attention\n",
    "\n",
    "path_to_extracted_features = '/omics/odcf/analysis/OE0585_projects/chromothripsis/histopathology/UKHD_Neuro/RetCLL_Features'\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L = 500\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "\n",
    "        # Features are already extracted\n",
    "        # self.feature_extractor_part1 = nn.Sequential(\n",
    "        #     nn.Conv2d(1, 20, kernel_size=5),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, stride=2),\n",
    "        #     nn.Conv2d(20, 50, kernel_size=5),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, stride=2)\n",
    "        # )\n",
    "        \n",
    "        # Features come in at 2048 per patch\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(2048, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.squeeze(0)\n",
    "\n",
    "        # H = self.feature_extractor_part1(x)\n",
    "        H = self.feature_extractor_part2(x)  # NxL\n",
    "\n",
    "        A = self.attention(H)  # NxK\n",
    "        A = torch.transpose(A, 2, 1)  # KxN\n",
    "        A = F.softmax(A, dim=2)  # softmax over N\n",
    "\n",
    "        M = torch.matmul(A, H)  # KxL\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat, A\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat, _ = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().data.item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _, A = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "\n",
    "        return neg_log_likelihood.mean()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        loss = self.calculate_objective(x, y)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # z = self.encoder(x)\n",
    "        # x_hat = self.decoder(z)\n",
    "        # loss = nn.functional.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetCCLFeatureLoader(Dataset):\n",
    "    def __init__(self, slide_filenames, feature_path, labels, patches_per_iter = 10):\n",
    "        assert len(labels) == len(slide_filenames)\n",
    "        self.labels = labels\n",
    "        self.file_paths = [feature_path + \"/\" +  x for x in slide_filenames]\n",
    "        self.slide_names = slide_filenames\n",
    "        self.num_patches = patches_per_iter\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_path = self.file_paths[idx]\n",
    "        features = h5py.File(cur_path, 'r')['feats'][()]\n",
    "        label = self.labels[idx]\n",
    "        # features = features.reshape([1,-1,2048])\n",
    "        sampled_pchs = np.random.randint(0, features.shape[0], self.num_patches)\n",
    "        features = features[sampled_pchs,:]\n",
    "        return features.astype(np.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://www-int.dkfz-heidelberg.de:80\"\n",
    "os.environ['HTTPS_PROXY']=\"http://www-int.dkfz-heidelberg.de:80\"\n",
    "\n",
    "os.environ['TENSORBOARD_BINARY'] = '/home/p163v/mambaforge/envs/marugoto/bin/tensorboard'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "txt_idat\n",
       "10003886253_R02C02       Chromothripsis\n",
       "10003886253_R03C01    No Chromothripsis\n",
       "10003886256_R03C02    No Chromothripsis\n",
       "10003886258_R02C01    No Chromothripsis\n",
       "10003886259_R02C01    No Chromothripsis\n",
       "                            ...        \n",
       "9969477124_R05C02     No Chromothripsis\n",
       "9980102013_R06C01     No Chromothripsis\n",
       "9980102032_R03C01     No Chromothripsis\n",
       "9980102032_R04C01     No Chromothripsis\n",
       "9980102032_R05C01     No Chromothripsis\n",
       "Name: CT_class, Length: 1956, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_meta = pd.read_csv(\"../metadata/slides_FS_anno.csv\")\n",
    "ct_scoring = pd.read_csv(\"../metadata/CT_3_Class_Draft.csv\")\n",
    "\n",
    "\n",
    "\n",
    "ct_scoring[\"txt_idat\"] = ct_scoring[\"idat\"].astype(\"str\")\n",
    "ct_scoring.index = ct_scoring.txt_idat\n",
    "slide_meta.index = slide_meta.txt_idat\n",
    "ct_scoring = ct_scoring.drop(\"txt_idat\", axis=1)\n",
    "slide_meta = slide_meta.drop(\"txt_idat\", axis=1)\n",
    "slide_annots = slide_meta.join(ct_scoring, lsuffix=\"l\")\n",
    "\n",
    "\n",
    "myx = [x in [\"Chromothripsis\", \"No Chromothripsis\"] for x in slide_annots.CT_class]\n",
    "\n",
    "slide_annots = slide_annots.loc[myx]\n",
    "slide_names = slide_annots.uuid\n",
    "\n",
    "# slide_names\n",
    "slide_annots.CT_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the data\n",
    "\n",
    "files = [x + \".h5\" for x in slide_names]\n",
    "\n",
    "# with h5py.File(filename, \"r\") as f:\n",
    "#     # Print all root level object names (aka keys) \n",
    "#     # these can be group or dataset names \n",
    "#     print(\"Keys: %s\" % f.keys())\n",
    "#     # get first object name/key; may or may NOT be a group\n",
    "#     a_group_key = list(f.keys())[0]\n",
    "\n",
    "#     # get the object type for a_group_key: usually group or dataset\n",
    "#     print(type(f[a_group_key])) \n",
    "\n",
    "#     # If a_group_key is a group name, \n",
    "#     # this gets the object names in the group and returns as a list\n",
    "#     data = list(f[a_group_key])\n",
    "\n",
    "#     # If a_group_key is a dataset name, \n",
    "#     # this gets the dataset values and returns as a list\n",
    "#     data = list(f[a_group_key])\n",
    "#     # preferred methods to get dataset values:\n",
    "#     ds_obj = f[a_group_key]      # returns as a h5py dataset object\n",
    "#     ds_arr = f[a_group_key][()]  # returns as a numpy array\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myx = [os.path.exists(path_to_extracted_features + \"/\" + x) for x in files]\n",
    "files = np.array(files)[myx]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1529"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TilesPerPat = pd.read_csv(\"../metadata/TilesPerPat.csv\")\n",
    "filestokeep = TilesPerPat.loc[TilesPerPat[\"Tiles Per Slide\"]>=10].File.tolist()\n",
    "\n",
    "myx2 = [x in filestokeep for x in files]\n",
    "\n",
    "files = files[myx2]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = slide_annots.CT_class.factorize()[0][myx][myx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RetCCLDataset = RetCCLFeatureLoader(files, path_to_extracted_features,labels)\n",
    "x, y = RetCCLDataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data =  torch.utils.data.random_split(RetCCLDataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c0fd224916dd5602\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c0fd224916dd5602\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                    | Type       | Params\n",
      "-------------------------------------------------------\n",
      "0 | feature_extractor_part2 | Sequential | 1.0 M \n",
      "1 | attention               | Sequential | 64.3 K\n",
      "2 | classifier              | Sequential | 501   \n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.357     Total estimated model params size (MB)\n",
      "/home/p163v/mambaforge/envs/marugoto/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/p163v/mambaforge/envs/marugoto/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2708ab9a7d6f4c138071bff5c08c55ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Received SIGTERM: 15\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=10, log_every_n_steps=10) # limit_train_batches=100,\n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marugoto",
   "language": "python",
   "name": "marugoto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
